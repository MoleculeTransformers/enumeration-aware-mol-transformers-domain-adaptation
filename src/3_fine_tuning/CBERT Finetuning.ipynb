{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install deepchem sklearn matplotlib pandas sentence_transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note: These fine-tuning notebooks donot reproduce the exact results mentioned in the paper, please follow the settings in paper to reproduce the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_uuid": "96413d12d4ca227ace5c9c07766f4f317f89511b",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import KernelPCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import PredefinedSplit\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "from deepchem.molnet import load_clintox, load_tox21, load_bace_classification, load_bbbp\n",
    "import pandas as pd\n",
    "from data_reader import DataReader\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from numpy import ndarray\n",
    "import torch\n",
    "from torch import Tensor, device\n",
    "import transformers\n",
    "from ≈ import AutoModel, AutoTokenizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import normalize\n",
    "from typing import List, Dict, Tuple, Type, Union\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class CBERT(object):\n",
    "    \"\"\"\n",
    "    A class for embedding sentences, calculating similarities, and retriving sentences by SimCSE.\n",
    "    \"\"\"\n",
    "    def __init__(self, model_name_or_path: str, \n",
    "                device: str = None,\n",
    "                num_cells: int = 100,\n",
    "                num_cells_in_search: int = 10,\n",
    "                pooler = None):\n",
    "\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "        self.model = AutoModel.from_pretrained(model_name_or_path)\n",
    "        if device is None:\n",
    "            device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.device = device\n",
    "\n",
    "        self.index = None\n",
    "        self.is_faiss_index = False\n",
    "        self.num_cells = num_cells\n",
    "        self.num_cells_in_search = num_cells_in_search\n",
    "\n",
    "        if pooler is not None:\n",
    "            self.pooler = pooler\n",
    "        elif \"unsup\" in model_name_or_path:\n",
    "            logger.info(\"Use `cls_before_pooler` for unsupervised models. If you want to use other pooling policy, specify `pooler` argument.\")\n",
    "            self.pooler = \"cls_before_pooler\"\n",
    "        else:\n",
    "            self.pooler = \"cls\"\n",
    "    \n",
    "    def encode(self, sentence: Union[str, List[str]], \n",
    "                device: str = None, \n",
    "                return_numpy: bool = False,\n",
    "                normalize_to_unit: bool = True,\n",
    "                keepdim: bool = False,\n",
    "                batch_size: int = 64,\n",
    "                max_length: int = 128) -> Union[ndarray, Tensor]:\n",
    "        sentence = [str(smile) for smile in sentence]\n",
    "        target_device = self.device if device is None else device\n",
    "        self.model = self.model.to(target_device)\n",
    "        \n",
    "        single_sentence = False\n",
    "        if isinstance(sentence, str):\n",
    "            sentence = [sentence]\n",
    "            single_sentence = True\n",
    "\n",
    "        embedding_list = [] \n",
    "        with torch.no_grad():\n",
    "            total_batch = len(sentence) // batch_size + (1 if len(sentence) % batch_size > 0 else 0)\n",
    "            for batch_id in tqdm(range(total_batch)):\n",
    "                inputs = self.tokenizer(\n",
    "                    sentence[batch_id*batch_size:(batch_id+1)*batch_size], \n",
    "                    padding=True, \n",
    "                    truncation=True, \n",
    "                    max_length=max_length, \n",
    "                    return_tensors=\"pt\"\n",
    "                )\n",
    "                inputs = {k: v.to(target_device) for k, v in inputs.items()}\n",
    "                outputs = self.model(**inputs, return_dict=True)\n",
    "                if self.pooler == \"cls\":\n",
    "                    embeddings = outputs.pooler_output\n",
    "                elif self.pooler == \"cls_before_pooler\":\n",
    "                    embeddings = outputs.last_hidden_state[:, 0]\n",
    "                else:\n",
    "                    raise NotImplementedError\n",
    "                if normalize_to_unit:\n",
    "                    embeddings = embeddings / embeddings.norm(dim=1, keepdim=True)\n",
    "                embedding_list.append(embeddings.cpu())\n",
    "        embeddings = torch.cat(embedding_list, 0)\n",
    "        \n",
    "        if single_sentence and not keepdim:\n",
    "            embeddings = embeddings[0]\n",
    "        \n",
    "        if return_numpy and not isinstance(embeddings, ndarray):\n",
    "            return embeddings.numpy()\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at emtrl/simcse-smole-bert-muv-mlm and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# adjust the path to domain adapted encoder based on the domain adaptation dataset\n",
    "model_name_or_path = \"emtrl/simcse-smole-bert-muv-mlm\"\n",
    "encoder = CBERT(model_name_or_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(dataset_name):\n",
    "    dataset = DataReader(dataset_name)\n",
    "    (train_dataset, valid_dataset, test_dataset) = (dataset.train_dataset,\n",
    "                                                    dataset.valid_dataset, \n",
    "                                                    dataset.test_dataset\n",
    "                                                   )\n",
    "    X_train, y_train, X_valid, y_valid, X_test, y_test = (dataset.train_dataset.smiles,\n",
    "                        dataset.train_dataset.y,\n",
    "                        dataset.valid_dataset.smiles,\n",
    "                        dataset.valid_dataset.y,\n",
    "                        dataset.test_dataset.smiles,\n",
    "                        dataset.test_dataset.y,\n",
    "                       \n",
    "                       )\n",
    "    print(f\"Loading and embedding SMILES for dataset {dataset_name}\")\n",
    "    return (\n",
    "            encoder.encode(X_train), y_train,\n",
    "            encoder.encode(X_valid), y_valid,\n",
    "            encoder.encode(X_test), y_test\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_model(X_train, y_train, X_valid, y_valid, X_test, y_test):\n",
    "\n",
    "    # ps = PredefinedSplit(test_fold)\n",
    "    print(\"Training Classifier\")\n",
    "    parameters = {'estimator__class_weight':['balanced'],\n",
    "              'estimator__kernel':['rbf','sigmoid'], \n",
    "              'estimator__C':[1,0.5,0.25], 'estimator__gamma':['auto','scale']}\n",
    "    tox21_svc = GridSearchCV(OneVsRestClassifier(SVC(probability=True,\n",
    "                                                     random_state=23)), \n",
    "                             parameters, cv=3, scoring='roc_auc',n_jobs=-1)\n",
    "    result = tox21_svc.fit(X_train, y_train)\n",
    "    pred = tox21_svc.predict_proba(X_test)\n",
    "    pred_svc = np.copy(pred)\n",
    "    if len(np.array(y_test).shape) == 1 or np.array(y_test).shape[-1] == 1:\n",
    "        return roc_auc_score(y_test,pred[:,1])\n",
    "    else:\n",
    "        return roc_auc_score(y_test,pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_dataset(dataset_name):\n",
    "\n",
    "    X_train, y_train, X_valid, y_valid, X_test, y_test = load_dataset(dataset_name=dataset_name)\n",
    "    roc_score = train_and_evaluate_model(X_train, y_train, X_valid, y_valid, X_test, y_test)\n",
    "\n",
    "    print(f\"The AUROC score for dataset {dataset_name} is {roc_score:2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "0917ba0d9889194b44548e8e1d8f935f83b8c9eb"
   },
   "source": [
    "## Evaluate MoleculeNet Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and embedding SMILES for dataset clintox\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 19/19 [00:21<00:00,  1.13s/it]\n",
      "100%|█████████████████████████████████████████████| 3/3 [00:03<00:00,  1.04s/it]\n",
      "100%|█████████████████████████████████████████████| 3/3 [00:03<00:00,  1.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Classifier\n",
      "The AUROC score for dataset clintox is 0.981275\n",
      "\n",
      "****************************************************************************************************\n",
      "\n",
      "Loading and embedding SMILES for dataset bace\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 19/19 [00:17<00:00,  1.09it/s]\n",
      "100%|█████████████████████████████████████████████| 3/3 [00:02<00:00,  1.40it/s]\n",
      "100%|█████████████████████████████████████████████| 3/3 [00:03<00:00,  1.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Classifier\n",
      "The AUROC score for dataset bace is 0.776268\n",
      "\n",
      "****************************************************************************************************\n",
      "\n",
      "Loading and embedding SMILES for dataset bbbp\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 26/26 [00:26<00:00,  1.02s/it]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:04<00:00,  1.12s/it]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:04<00:00,  1.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Classifier\n",
      "The AUROC score for dataset bbbp is 0.712593\n",
      "\n",
      "****************************************************************************************************\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[04:38:04] WARNING: not removing hydrogen atom without neighbors\n",
      "[04:40:10] WARNING: not removing hydrogen atom without neighbors\n",
      "[04:40:19] WARNING: not removing hydrogen atom without neighbors\n",
      "[04:42:28] WARNING: not removing hydrogen atom without neighbors\n",
      "[04:42:37] WARNING: not removing hydrogen atom without neighbors\n",
      "[04:44:45] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and embedding SMILES for dataset tox21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 98/98 [01:12<00:00,  1.36it/s]\n",
      " 69%|██████████████████████████████▍             | 9/13 [00:09<00:04,  1.05s/it]"
     ]
    }
   ],
   "source": [
    "evaluate_dataset(dataset_name=\"clintox\")\n",
    "print(f\"\\n{'*'*100}\\n\")\n",
    "evaluate_dataset(dataset_name=\"bace\")\n",
    "print(f\"\\n{'*'*100}\\n\")\n",
    "evaluate_dataset(dataset_name=\"bbbp\")\n",
    "print(f\"\\n{'*'*100}\\n\")\n",
    "evaluate_dataset(dataset_name=\"tox21\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
