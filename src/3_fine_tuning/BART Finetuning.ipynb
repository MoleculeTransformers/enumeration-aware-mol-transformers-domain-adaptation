{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install deepchem sklearn matplotlib pandas sentence_transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note: These fine-tuning notebooks donot reproduce the exact results mentioned in the paper, please follow the settings in paper to reproduce the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_uuid": "96413d12d4ca227ace5c9c07766f4f317f89511b",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import KernelPCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import PredefinedSplit\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "from deepchem.molnet import load_clintox, load_tox21, load_bace_classification, load_bbbp\n",
    "import pandas as pd\n",
    "from data_reader import DataReader\n",
    "import numpy as np\n",
    "from typing import List, Union\n",
    "from transformers import BartModel, BartTokenizer\n",
    "from torch import Tensor\n",
    "from numpy import ndarray\n",
    "from tqdm import tqdm\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BartFeaturizer:\n",
    "    def __init__(self, model_name_or_path: str):\n",
    "        self.tokenizer = BartTokenizer.from_pretrained(model_name_or_path)\n",
    "        self.model = BartModel.from_pretrained(model_name_or_path)\n",
    "\n",
    "    def encode(\n",
    "        self, smiles: Union[str, List[str]], embedder: str = \"encoder\", batch_size=8\n",
    "    ) -> Union[List[Tensor], ndarray, Tensor]:\n",
    "        assert len(smiles) > 0, \"SMILES can not be empty!\"\n",
    "        smiles = [str(smile) for smile in smiles]\n",
    "        def batch(iterable, n=1):\n",
    "            l = len(iterable)\n",
    "            for ndx in range(0, l, n):\n",
    "                yield iterable[ndx:min(ndx + n, l)]\n",
    "        embeddings = []\n",
    "        for smiles_batch in tqdm(batch(smiles, batch_size)):\n",
    "            inputs = self.tokenizer(smiles_batch, return_tensors=\"pt\", padding=True)\n",
    "            outputs = self.model(**inputs)\n",
    "            if embedder == \"encoder\":\n",
    "                embeddings += outputs.encoder_last_hidden_state.mean(dim=1).tolist()\n",
    "            elif embedder == \"decoder\":\n",
    "                embeddings += outputs.last_hidden_state.mean(dim=1).tolist()\n",
    "            else:\n",
    "                raise NotImplementedError\n",
    "\n",
    "        return np.array(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adjust the path to domain adapted encoder based on the domain adaptation dataset\n",
    "model_name_or_path = \"emtrl/smole-bart\"\n",
    "encoder = BartFeaturizer(model_name_or_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(dataset_name):\n",
    "    dataset = DataReader(dataset_name)\n",
    "    (train_dataset, valid_dataset, test_dataset) = (dataset.train_dataset,\n",
    "                                                    dataset.valid_dataset, \n",
    "                                                    dataset.test_dataset\n",
    "                                                   )\n",
    "    X_train, y_train, X_valid, y_valid, X_test, y_test = (dataset.train_dataset.smiles,\n",
    "                        dataset.train_dataset.y,\n",
    "                        dataset.valid_dataset.smiles,\n",
    "                        dataset.valid_dataset.y,\n",
    "                        dataset.test_dataset.smiles,\n",
    "                        dataset.test_dataset.y,\n",
    "                       \n",
    "                       )\n",
    "    print(f\"Loading and embedding SMILES for dataset {dataset_name}\")\n",
    "    return (\n",
    "            encoder.encode(X_train), y_train,\n",
    "            encoder.encode(X_valid), y_valid,\n",
    "            encoder.encode(X_test), y_test\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_model(X_train, y_train, X_valid, y_valid, X_test, y_test):\n",
    "\n",
    "    # ps = PredefinedSplit(test_fold)\n",
    "    print(\"Training Classifier\")\n",
    "    parameters = {'estimator__class_weight':['balanced'],\n",
    "              'estimator__kernel':['rbf','sigmoid'], \n",
    "              'estimator__C':[1,0.5,0.25], 'estimator__gamma':['auto','scale']}\n",
    "    tox21_svc = GridSearchCV(OneVsRestClassifier(SVC(probability=True,\n",
    "                                                     random_state=23)), \n",
    "                             parameters, cv=3, scoring='roc_auc',n_jobs=-1)\n",
    "    result = tox21_svc.fit(X_train, y_train)\n",
    "    pred = tox21_svc.predict_proba(X_test)\n",
    "    pred_svc = np.copy(pred)\n",
    "    if len(np.array(y_test).shape) == 1 or np.array(y_test).shape[-1] == 1:\n",
    "        return roc_auc_score(y_test,pred[:,1])\n",
    "    else:\n",
    "        return roc_auc_score(y_test,pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_dataset(dataset_name):\n",
    "\n",
    "    X_train, y_train, X_valid, y_valid, X_test, y_test = load_dataset(dataset_name=dataset_name)\n",
    "    roc_score = train_and_evaluate_model(X_train, y_train, X_valid, y_valid, X_test, y_test)\n",
    "\n",
    "    print(f\"The AUROC score for dataset {dataset_name} is {roc_score:2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "0917ba0d9889194b44548e8e1d8f935f83b8c9eb"
   },
   "source": [
    "## Evaluate MoleculeNet Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and embedding SMILES for dataset clintox\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "148it [00:03, 37.21it/s]\n",
      "19it [00:00, 31.46it/s]\n",
      "19it [00:00, 41.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Classifier\n",
      "The AUROC score for dataset clintox is 0.985660\n",
      "\n",
      "****************************************************************************************************\n",
      "\n",
      "Loading and embedding SMILES for dataset bace\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "152it [00:03, 49.14it/s]\n",
      "19it [00:00, 46.97it/s]\n",
      "19it [00:00, 38.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Classifier\n",
      "The AUROC score for dataset bace is 0.746920\n",
      "\n",
      "****************************************************************************************************\n",
      "\n",
      "Loading and embedding SMILES for dataset bbbp\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "204it [00:03, 53.07it/s]\n",
      "26it [00:00, 32.13it/s]\n",
      "26it [00:00, 39.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Classifier\n",
      "The AUROC score for dataset bbbp is 0.689758\n",
      "\n",
      "****************************************************************************************************\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[04:16:19] WARNING: not removing hydrogen atom without neighbors\n",
      "[04:16:47] WARNING: not removing hydrogen atom without neighbors\n",
      "[04:16:49] WARNING: not removing hydrogen atom without neighbors\n",
      "[04:17:17] WARNING: not removing hydrogen atom without neighbors\n",
      "[04:17:20] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    }
   ],
   "source": [
    "evaluate_dataset(dataset_name=\"clintox\")\n",
    "print(f\"\\n{'*'*100}\\n\")\n",
    "evaluate_dataset(dataset_name=\"bace\")\n",
    "print(f\"\\n{'*'*100}\\n\")\n",
    "evaluate_dataset(dataset_name=\"bbbp\")\n",
    "print(f\"\\n{'*'*100}\\n\")\n",
    "evaluate_dataset(dataset_name=\"tox21\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
