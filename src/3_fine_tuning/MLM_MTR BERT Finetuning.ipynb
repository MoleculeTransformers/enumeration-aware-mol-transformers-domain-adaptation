{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install deepchem sklearn matplotlib pandas sentence_transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note: These fine-tuning notebooks donot reproduce the exact results mentioned in the paper, please follow the settings in paper to reproduce the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_uuid": "96413d12d4ca227ace5c9c07766f4f317f89511b",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import KernelPCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import PredefinedSplit\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "from deepchem.molnet import load_clintox, load_tox21, load_bace_classification, load_bbbp\n",
    "import pandas as pd\n",
    "from data_reader import DataReader\n",
    "import numpy as np\n",
    "from typing import List, Union\n",
    "from transformers import BertModel, BertTokenizer\n",
    "from torch import Tensor\n",
    "from numpy import ndarray\n",
    "from tqdm import tqdm\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads both MLM and MTR BERT pre-trained models\n",
    "class BertFeaturizer:\n",
    "    def __init__(self, model_name_or_path: str):\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(model_name_or_path)\n",
    "        self.model = BertModel.from_pretrained(model_name_or_path)\n",
    "\n",
    "    def encode(\n",
    "        self, smiles: Union[str, List[str]], embedder: str = \"encoder\", batch_size=8\n",
    "    ) -> Union[List[Tensor], ndarray, Tensor]:\n",
    "        assert len(smiles) > 0, \"SMILES can not be empty!\"\n",
    "        smiles = [str(smile) for smile in smiles]\n",
    "\n",
    "        def batch(iterable, n=1):\n",
    "            l = len(iterable)\n",
    "            for ndx in range(0, l, n):\n",
    "                yield iterable[ndx:min(ndx + n, l)]\n",
    "        embeddings = []\n",
    "        for smiles_batch in tqdm(batch(smiles, batch_size)):\n",
    "            inputs = self.tokenizer(smiles_batch, return_tensors=\"pt\", padding=True)\n",
    "            outputs = self.model(**inputs, output_hidden_states=True)\n",
    "            embeddings += outputs.hidden_states[-1].mean(dim=1).tolist()\n",
    "\n",
    "        return np.array(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(dataset_name):\n",
    "    dataset = DataReader(dataset_name)\n",
    "    (train_dataset, valid_dataset, test_dataset) = (dataset.train_dataset,\n",
    "                                                    dataset.valid_dataset, \n",
    "                                                    dataset.test_dataset\n",
    "                                                   )\n",
    "    X_train, y_train, X_valid, y_valid, X_test, y_test = (dataset.train_dataset.smiles,\n",
    "                        dataset.train_dataset.y,\n",
    "                        dataset.valid_dataset.smiles,\n",
    "                        dataset.valid_dataset.y,\n",
    "                        dataset.test_dataset.smiles,\n",
    "                        dataset.test_dataset.y,\n",
    "                       \n",
    "                       )\n",
    "    print(f\"Loading and embedding SMILES for dataset {dataset_name}\")\n",
    "    return (\n",
    "            encoder.encode(X_train), y_train,\n",
    "            encoder.encode(X_valid), y_valid,\n",
    "            encoder.encode(X_test), y_test\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_model(X_train, y_train, X_valid, y_valid, X_test, y_test):\n",
    "\n",
    "    # ps = PredefinedSplit(test_fold)\n",
    "    print(\"Training Classifier\")\n",
    "    parameters = {'estimator__class_weight':['balanced'],\n",
    "              'estimator__kernel':['rbf','sigmoid'], \n",
    "              'estimator__C':[1,0.5,0.25], 'estimator__gamma':['auto','scale']}\n",
    "    tox21_svc = GridSearchCV(OneVsRestClassifier(SVC(probability=True,\n",
    "                                                     random_state=23)), \n",
    "                             parameters, cv=3, scoring='roc_auc',n_jobs=-1)\n",
    "    result = tox21_svc.fit(X_train, y_train)\n",
    "    pred = tox21_svc.predict_proba(X_test)\n",
    "    pred_svc = np.copy(pred)\n",
    "    if len(np.array(y_test).shape) == 1 or np.array(y_test).shape[-1] == 1:\n",
    "        return roc_auc_score(y_test,pred[:,1])\n",
    "    else:\n",
    "        return roc_auc_score(y_test,pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_dataset(dataset_name):\n",
    "\n",
    "    X_train, y_train, X_valid, y_valid, X_test, y_test = load_dataset(dataset_name=dataset_name)\n",
    "    roc_score = train_and_evaluate_model(X_train, y_train, X_valid, y_valid, X_test, y_test)\n",
    "\n",
    "    print(f\"The AUROC score for dataset {dataset_name} is {roc_score:2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "0917ba0d9889194b44548e8e1d8f935f83b8c9eb"
   },
   "source": [
    "## Evaluate MoleculeNet Datasets with MTR Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at emtrl/smole-bert-mtr and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# adjust the path to domain adapted encoder based on the domain adaptation dataset\n",
    "model_name_or_path = \"emtrl/smole-bert-mtr\"\n",
    "encoder = BertFeaturizer(model_name_or_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and embedding SMILES for dataset clintox\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "148it [00:24,  5.95it/s]\n",
      "19it [00:03,  4.82it/s]\n",
      "19it [00:02,  6.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Classifier\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The AUROC score for dataset clintox is 0.965422\n",
      "\n",
      "****************************************************************************************************\n",
      "\n",
      "Loading and embedding SMILES for dataset bace\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "152it [00:19,  7.61it/s]\n",
      "19it [00:02,  7.72it/s]\n",
      "19it [00:03,  5.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Classifier\n",
      "The AUROC score for dataset bace is 0.805797\n",
      "\n",
      "****************************************************************************************************\n",
      "\n",
      "Loading and embedding SMILES for dataset bbbp\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "204it [00:25,  7.91it/s]\n",
      "26it [00:05,  4.43it/s]\n",
      "26it [00:05,  4.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Classifier\n",
      "The AUROC score for dataset bbbp is 0.768957\n",
      "\n",
      "****************************************************************************************************\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[04:22:01] WARNING: not removing hydrogen atom without neighbors\n",
      "[04:22:59] WARNING: not removing hydrogen atom without neighbors\n",
      "[04:23:04] WARNING: not removing hydrogen atom without neighbors\n",
      "[04:24:06] WARNING: not removing hydrogen atom without neighbors\n",
      "[04:24:10] WARNING: not removing hydrogen atom without neighbors\n",
      "[04:25:06] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and embedding SMILES for dataset tox21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "783it [01:40,  7.80it/s]\n",
      "98it [00:19,  4.92it/s]\n",
      "23it [00:04,  4.43it/s]"
     ]
    }
   ],
   "source": [
    "evaluate_dataset(dataset_name=\"clintox\")\n",
    "print(f\"\\n{'*'*100}\\n\")\n",
    "evaluate_dataset(dataset_name=\"bace\")\n",
    "print(f\"\\n{'*'*100}\\n\")\n",
    "evaluate_dataset(dataset_name=\"bbbp\")\n",
    "print(f\"\\n{'*'*100}\\n\")\n",
    "evaluate_dataset(dataset_name=\"tox21\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate MoleculeNet Datasets with MLM Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adjust the path to domain adapted encoder based on the domain adaptation dataset\n",
    "model_name_or_path = \"emtrl/smole-bert\"\n",
    "encoder = BertFeaturizer(model_name_or_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_dataset(dataset_name=\"clintox\")\n",
    "print(f\"\\n{'*'*100}\\n\")\n",
    "evaluate_dataset(dataset_name=\"bace\")\n",
    "print(f\"\\n{'*'*100}\\n\")\n",
    "evaluate_dataset(dataset_name=\"bbbp\")\n",
    "print(f\"\\n{'*'*100}\\n\")\n",
    "evaluate_dataset(dataset_name=\"tox21\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
